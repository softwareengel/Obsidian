---
title: transformer.js
tags: 
date: 2024-06-06
---

# transformer.js
<https://github.com/xenova/transformers.js?tab=readme-ov-file>

State-of-the-art Machine Learning for the web. Run ğŸ¤— Transformers directly in your browser, with no need for a server!

Transformers.js is designed to be functionally equivalent to Hugging Face'sÂ [transformers](https://github.com/huggingface/transformers)Â python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as:

- ğŸ“Â **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.
- ğŸ–¼ï¸Â **Computer Vision**: image classification, object detection, and segmentation.
- ğŸ—£ï¸Â **Audio**: automatic speech recognition and audio classification.
- ğŸ™Â **Multimodal**: zero-shot image classification.

Transformers.js usesÂ [ONNX Runtime](https://onnxruntime.ai/)Â to run models in the browser. The best part about it, is that you can easilyÂ [convert](https://github.com/xenova/transformers.js?tab=readme-ov-file#convert-your-models-to-onnx)Â your pretrained PyTorch, TensorFlow, or JAX models to ONNX usingÂ [ğŸ¤— Optimum](https://github.com/huggingface/optimum#onnx--onnx-runtime).

For more information, check out the fullÂ [documentation](https://huggingface.co/docs/transformers.js).

## Beispiel

![](../_asset/2024-04-11-transformer.js_image_1.png)


<figure class="video_container">
  <video id="myVideo" width="100%"  controls="true" allowfullscreen="true" autoplay poster="../_asset/2024-04-11-transformer.js_video_1.mp4">
    <source src="../_asset/2024-04-11-transformer.js_video_1.mp4" type="video/mp4">
  </video>
</figure>

<script> document.addEventListener('DOMContentLoaded', (event) => { var video = document.getElementById('myVideo'); video.currentTime = 5; video.play(); }); </script>

![](../_asset/2024-04-11-transformer.js_video_1.mp4)
